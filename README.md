ğŸŒŠ InsightFlow â€“ Role-Based AI Assistant:

InsightFlow is an intelligent conversational assistant that delivers role-specific responses based on hierarchical access. It blends FastAPI for backend logic, LangChain + FAISS (or pluggable vector stores) for semantic search, and Streamlit for a seamless chat UI experience.

ğŸ’» Tech Stack:

ğŸ’» Tech Stack
- Python â€“ The core engine driving backend logic and orchestration.
- FastAPI â€“ Powers the authentication layer and serves chat endpoints.
- LangChain â€“ Enables role-aware prompting through RetrievalQA and custom chains.
- LLMs (Claude, GPT-4, etc.) â€“ Generates natural language responses with citation-ready accuracy.
- Vector Stores (FAISS / Qdrant / Chroma / Pinecone) â€“ Embed and retrieve relevant document chunks via semantic similarity.
- Streamlit â€“ Delivers a chat-first UI with login, chat memory, and interactive frontend behaviors.
- Markdown (.md) â€“ Used to organize role-based knowledge files, making documentation easy to manage and update.

ğŸ’¡ Some resource structure and design inspiration derived from Codebasics open-source projects.

ğŸ—ï¸ Project Structure:


BOT/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                # FastAPI application with chat endpoints
â”‚   â”œâ”€â”€ auth_service.py        # Authentication using HTTPBasic with role/c_level
â”‚   â”œâ”€â”€ chat_service.py        # Query resolution logic (get_answer)
â”œâ”€â”€ schema/
â”‚   â”œâ”€â”€ models.py              # Pydantic models for request/response bodies
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ document_loader.py     # Load .md files tagged with role/c_level
â”‚   â”œâ”€â”€ retrieval_chain.py     # Prompt template + LangChain QA chain
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ vector_utils.py        # FAISS index creation and retrieval
â”œâ”€â”€ resources/
â”‚   â””â”€â”€ roles/                 # Role-based markdown content
â”œâ”€â”€ faiss_index/               # Serialized FAISS vectorstore
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ app.py                 # Streamlit frontend with login, chat, logout
â”œâ”€â”€ .env                       # Environment variables (e.g. API keys)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md                  # Project documentation
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml


ğŸš€ How It Works:

Login System: Authenticated via FastAPIâ€™s HTTPBasic. Each user has a role and C-level access.

Role-based Document Retrieval: .md files stored by role â†’ vectorized â†’ indexed.

Dynamic Prompting: Prompt template adjusts to user's role and clearance.

LLM-Powered Answers: Using Claude or any other LLM with LangChain RetrievalQA.

Chat UI: Streamlit-powered interface for seamless interaction.



ğŸ§ª Local Development Setup:

# Clone the repo
git clone https://github.com/your-repo/insightflow.git
cd insightflow
# Install dependencies
install on based dependencies by using requirements.txt
# Set up environment variables
# (Create a .env file and add your claude API key)
echo "ANTHROPIC_API_KEY=your_api_key_here" > .env

# Run backend
uvicorn app.main:app --reload
# Run frontend
streamlit run ui/app.py


ğŸ”‘ Key Features
ğŸ” Role-Based Access Control
Users are authenticated through FastAPI and assigned a role (like engineer, ceo, or hr). InsightFlow dynamically filters what knowledge a user can access and tailors responses accordingly. C-level roles receive higher-tier insights, while general access is scoped appropriatelyâ€”this helps ensure both relevance and security.
ğŸ“„ Editable Markdown Knowledge Base
Content is stored as .md files, categorized by role. This makes the system incredibly flexibleâ€”non-tech contributors can update knowledge sources, and the assistant automatically adapts. Markdown is clean, maintainable, and integrates seamlessly into the indexing pipeline.
ğŸ§  LangChain-Driven Prompt Engineering
At the heart of InsightFlow is a dynamic prompting system. Based on the userâ€™s role and query, LangChain constructs custom prompts that guide the LLM to retrieve relevant content. The system balances contextual depth and brevity for optimal usability.
ğŸ” Semantic Search with MMR Filtering
Using FAISS (or a pluggable vector store), the system indexes role-based docs and performs fast, intelligent lookups. Max Marginal Relevance ensures diversity and avoids echo-chamber responses by selecting the most relevant and varied document chunks.
ğŸ’¬ Streamlit Chat UI with Memory & Auth
The front-endâ€”built entirely in Streamlitâ€”features login/logout logic, chat history memory, and a clean UI that's easily styled with custom CSS/HTML. Itâ€™s snappy, secure, and designed for long-form conversations that adapt as you go.
âš¡ Pluggable Design
From vector stores to UI theming, everything is modular. Want to swap out FAISS for Qdrant or deploy to another UI framework? Minimal refactoring is needed. It's developer-friendly, extensible, and ready for upgrades.




âœ¨ further improvement ideas

OAuth2 or JWT authentication

Admin portal to upload/manage role content

Analytics dashboard for usage and insights

ğŸ¤ Author
Developed by Navadeep â€” exploring scalable, secure, and intelligent systems with a flair for clean UI.

 Resources provided by codebasics.